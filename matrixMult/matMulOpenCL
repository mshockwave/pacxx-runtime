; ModuleID = 'main'
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-pc-linux"

; Function Attrs: nounwind
declare void @__MatrixMultKernel_before.AddImplicitArgs(i32 addrspace(1)* nocapture readonly, i32 addrspace(1)* nocapture readonly, i32 addrspace(1)* nocapture, i32) #0

; Function Attrs: nounwind readnone
declare i64 @_Z13get_global_idj(i32) #1

declare [7 x i64] @__WG.boundaries.MatrixMultKernel_before.AddImplicitArgs(i32 addrspace(1)*, i32 addrspace(1)*, i32 addrspace(1)*, i32)

declare i64 @_Z14get_local_sizej(i32)

declare i64 @get_base_global_id.(i32)

declare i1 @__ocl_allOne(i1)

declare i1 @__ocl_allZero(i1)

; Function Attrs: alwaysinline nounwind
declare void @__MatrixMultKernel_separated_args(i32 addrspace(1)* nocapture readonly, i32 addrspace(1)* nocapture readonly, i32 addrspace(1)* nocapture, i32, i8 addrspace(3)* noalias, { i64, [3 x i64], [3 x i64], [2 x [3 x i64]], [3 x i64], {}*, {}* }* noalias, i64* noalias, [4 x i64], i8* noalias, {}* noalias) #2

declare [7 x i64] @WG.boundaries.MatrixMultKernel(i32 addrspace(1)*, i32 addrspace(1)*, i32 addrspace(1)*, i32, i8 addrspace(3)* noalias, { i64, [3 x i64], [3 x i64], [2 x [3 x i64]], [3 x i64], {}*, {}* }* noalias, i64* noalias, [4 x i64], i8* noalias, {}* noalias)

define void @MatrixMultKernel(i8* noalias %pUniformArgs, i64* noalias %pWGId, {}* noalias %RuntimeHandle) {
wrapper_entry:
  %0 = bitcast i8* %pUniformArgs to i32 addrspace(1)**
  %explicit_0 = load i32 addrspace(1)** %0, align 8
  %1 = getelementptr i8* %pUniformArgs, i64 8
  %2 = bitcast i8* %1 to i32 addrspace(1)**
  %explicit_1 = load i32 addrspace(1)** %2, align 8
  %3 = getelementptr i8* %pUniformArgs, i64 16
  %4 = bitcast i8* %3 to i32 addrspace(1)**
  %explicit_2 = load i32 addrspace(1)** %4, align 8
  %5 = getelementptr i8* %pUniformArgs, i64 24
  %6 = bitcast i8* %5 to i32*
  %explicit_3 = load i32* %6, align 4
  %7 = sext i32 %explicit_3 to i64
  %8 = sext i32 %explicit_3 to i64
  %9 = getelementptr i8* %pUniformArgs, i64 88
  %10 = bitcast i8* %9 to i64*
  %LocalSize_0 = load i64* %10, align 8
  %11 = getelementptr i8* %pUniformArgs, i64 96
  %12 = bitcast i8* %11 to i64*
  %LocalSize_1 = load i64* %12, align 8
  %13 = getelementptr i8* %pUniformArgs, i64 40
  %14 = bitcast i8* %13 to i64*
  %GlobalOffset_0 = load i64* %14, align 8
  %15 = getelementptr i8* %pUniformArgs, i64 48
  %16 = bitcast i8* %15 to i64*
  %GlobalOffset_1 = load i64* %16, align 8
  %GroupID_0 = load i64* %pWGId, align 8
  %17 = getelementptr i64* %pWGId, i64 1
  %GroupID_1 = load i64* %17, align 8
  %18 = mul i64 %LocalSize_0, %GroupID_0
  %19 = add i64 %18, %GlobalOffset_0
  %20 = mul i64 %LocalSize_1, %GroupID_1
  %21 = add i64 %20, %GlobalOffset_1
  %vector.size.i = ashr i64 %LocalSize_0, 2
  %num.vector.wi.i = shl nsw i64 %vector.size.i, 2
  %max.vector.gid.i = add i64 %num.vector.wi.i, %19
  %scalar.size.i = sub i64 %LocalSize_0, %num.vector.wi.i
  %22 = icmp eq i64 %vector.size.i, 0
  br i1 %22, label %scalarIf.i, label %dim_1_vector_pre_head.i

dim_1_vector_pre_head.i:                          ; preds = %wrapper_entry
  %23 = mul i64 %8, 4
  %24 = mul i64 %LocalSize_1, %GroupID_1
  %25 = add i64 %GlobalOffset_1, %24
  %26 = trunc i64 %25 to i32
  %27 = mul i32 %explicit_3, %26
  br label %dim_0_vector_pre_head.i

dim_0_vector_pre_head.i:                          ; preds = %dim_0_vector_exit.i, %dim_1_vector_pre_head.i
  %lsr.iv21 = phi i32 [ %lsr.iv.next22, %dim_0_vector_exit.i ], [ %27, %dim_1_vector_pre_head.i ]
  %dim_1_vector_ind_var.i = phi i64 [ 0, %dim_1_vector_pre_head.i ], [ %dim_1_vector_inc_ind_var.i, %dim_0_vector_exit.i ]
  %dim_1_vector_tid.i = phi i64 [ %21, %dim_1_vector_pre_head.i ], [ %dim_1_vector_inc_tid.i, %dim_0_vector_exit.i ]
  %28 = sext i32 %lsr.iv21 to i64
  %scevgep23 = getelementptr i32 addrspace(1)* %explicit_0, i64 %28
  %29 = trunc i64 %dim_1_vector_tid.i to i32
  %30 = mul nsw i32 %29, %explicit_3
  br label %31

; <label>:31                                      ; preds = %._crit_edgevector_func.i, %dim_0_vector_pre_head.i
  %dim_0_vector_ind_var.i = phi i64 [ 0, %dim_0_vector_pre_head.i ], [ %dim_0_vector_inc_ind_var.i, %._crit_edgevector_func.i ]
  %dim_0_vector_tid.i = phi i64 [ %19, %dim_0_vector_pre_head.i ], [ %dim_0_vector_inc_tid.i, %._crit_edgevector_func.i ]
  %32 = icmp sgt i32 %explicit_3, 0
  %lsr26 = trunc i64 %dim_0_vector_tid.i to i32
  %33 = sext i32 %lsr26 to i64
  %scevgep16 = getelementptr i32 addrspace(1)* %explicit_1, i64 %33
  br i1 %32, label %34, label %._crit_edgevector_func.i

; <label>:34                                      ; preds = %31, %34
  %lsr.iv24 = phi i32 addrspace(1)* [ %scevgep25, %34 ], [ %scevgep23, %31 ]
  %lsr.iv17 = phi i32 addrspace(1)* [ %39, %34 ], [ %scevgep16, %31 ]
  %lsr.iv12 = phi i32 [ %lsr.iv.next13, %34 ], [ %explicit_3, %31 ]
  %vectorPHIvector_func.i = phi <4 x i32> [ %38, %34 ], [ zeroinitializer, %31 ]
  %lsr.iv1720 = bitcast i32 addrspace(1)* %lsr.iv17 to <4 x i32> addrspace(1)*
  %lsr.iv1718 = bitcast i32 addrspace(1)* %lsr.iv17 to i1 addrspace(1)*
  %35 = load i32 addrspace(1)* %lsr.iv24, align 1, !noalias !31
  %temp8vector_func.i = insertelement <4 x i32> undef, i32 %35, i32 0
  %vector7vector_func.i = shufflevector <4 x i32> %temp8vector_func.i, <4 x i32> undef, <4 x i32> zeroinitializer
  %36 = load <4 x i32> addrspace(1)* %lsr.iv1720, align 1, !noalias !31
  %37 = mul nsw <4 x i32> %36, %vector7vector_func.i
  %38 = add nsw <4 x i32> %37, %vectorPHIvector_func.i
  %lsr.iv.next13 = add i32 %lsr.iv12, -1
  %scevgep19 = getelementptr i1 addrspace(1)* %lsr.iv1718, i64 %23
  %39 = bitcast i1 addrspace(1)* %scevgep19 to i32 addrspace(1)*
  %scevgep25 = getelementptr i32 addrspace(1)* %lsr.iv24, i64 1
  %exitcondvector_func.i = icmp eq i32 %lsr.iv.next13, 0
  br i1 %exitcondvector_func.i, label %._crit_edgevector_func.i, label %34

._crit_edgevector_func.i:                         ; preds = %34, %31
  %vectorPHI9vector_func.i = phi <4 x i32> [ zeroinitializer, %31 ], [ %38, %34 ]
  %extract12.rhsvector_func.i = trunc i64 %dim_0_vector_tid.i to i32
  %extract12vector_func.i = add i32 %30, %extract12.rhsvector_func.i
  %40 = sext i32 %extract12vector_func.i to i64
  %41 = getelementptr inbounds i32 addrspace(1)* %explicit_2, i64 %40
  %ptrTypeCast16vector_func.i = bitcast i32 addrspace(1)* %41 to <4 x i32> addrspace(1)*
  store <4 x i32> %vectorPHI9vector_func.i, <4 x i32> addrspace(1)* %ptrTypeCast16vector_func.i, align 1, !noalias !31
  %dim_0_vector_inc_ind_var.i = add nuw nsw i64 %dim_0_vector_ind_var.i, 1
  %dim_0_vector_inc_tid.i = add nuw nsw i64 %dim_0_vector_tid.i, 4
  %dim_0_vector_cmp.to.max.i = icmp eq i64 %dim_0_vector_inc_ind_var.i, %vector.size.i
  br i1 %dim_0_vector_cmp.to.max.i, label %dim_0_vector_exit.i, label %31

dim_0_vector_exit.i:                              ; preds = %._crit_edgevector_func.i
  %dim_1_vector_inc_ind_var.i = add nuw nsw i64 %dim_1_vector_ind_var.i, 1
  %dim_1_vector_inc_tid.i = add nuw nsw i64 %dim_1_vector_tid.i, 1
  %lsr.iv.next22 = add i32 %lsr.iv21, %explicit_3
  %dim_1_vector_cmp.to.max.i = icmp eq i64 %dim_1_vector_inc_ind_var.i, %LocalSize_1
  br i1 %dim_1_vector_cmp.to.max.i, label %scalarIf.i, label %dim_0_vector_pre_head.i

scalarIf.i:                                       ; preds = %dim_0_vector_exit.i, %wrapper_entry
  %42 = icmp eq i64 %LocalSize_0, %num.vector.wi.i
  br i1 %42, label %__MatrixMultKernel_separated_args.exit, label %dim_1_pre_head.i

dim_1_pre_head.i:                                 ; preds = %scalarIf.i
  %43 = mul i64 %7, 4
  %44 = mul i64 %LocalSize_1, %GroupID_1
  %45 = add i64 %GlobalOffset_1, %44
  %46 = trunc i64 %45 to i32
  %47 = mul i32 %explicit_3, %46
  br label %dim_0_pre_head.i

dim_0_pre_head.i:                                 ; preds = %dim_0_exit.i, %dim_1_pre_head.i
  %lsr.iv7 = phi i32 [ %lsr.iv.next8, %dim_0_exit.i ], [ %47, %dim_1_pre_head.i ]
  %dim_1_ind_var.i = phi i64 [ 0, %dim_1_pre_head.i ], [ %dim_1_inc_ind_var.i, %dim_0_exit.i ]
  %dim_1_tid.i = phi i64 [ %21, %dim_1_pre_head.i ], [ %dim_1_inc_tid.i, %dim_0_exit.i ]
  %48 = sext i32 %lsr.iv7 to i64
  %scevgep9 = getelementptr i32 addrspace(1)* %explicit_0, i64 %48
  %49 = trunc i64 %dim_1_tid.i to i32
  %50 = mul nsw i32 %49, %explicit_3
  br label %scalar_kernel_entry.i

scalar_kernel_entry.i:                            ; preds = %._crit_edge.i, %dim_0_pre_head.i
  %dim_0_ind_var.i = phi i64 [ 0, %dim_0_pre_head.i ], [ %dim_0_inc_ind_var.i, %._crit_edge.i ]
  %dim_0_tid.i = phi i64 [ %max.vector.gid.i, %dim_0_pre_head.i ], [ %dim_0_inc_tid.i, %._crit_edge.i ]
  %51 = icmp sgt i32 %explicit_3, 0
  %lsr = trunc i64 %dim_0_tid.i to i32
  %52 = sext i32 %lsr to i64
  %scevgep = getelementptr i32 addrspace(1)* %explicit_1, i64 %52
  %53 = trunc i64 %dim_0_tid.i to i32
  br i1 %51, label %54, label %._crit_edge.i

; <label>:54                                      ; preds = %scalar_kernel_entry.i, %54
  %lsr.iv10 = phi i32 addrspace(1)* [ %scevgep11, %54 ], [ %scevgep9, %scalar_kernel_entry.i ]
  %lsr.iv4 = phi i32 addrspace(1)* [ %59, %54 ], [ %scevgep, %scalar_kernel_entry.i ]
  %lsr.iv = phi i32 [ %lsr.iv.next, %54 ], [ %explicit_3, %scalar_kernel_entry.i ]
  %sum.01.i = phi i32 [ %58, %54 ], [ 0, %scalar_kernel_entry.i ]
  %lsr.iv45 = bitcast i32 addrspace(1)* %lsr.iv4 to i1 addrspace(1)*
  %55 = load i32 addrspace(1)* %lsr.iv10, align 1, !noalias !31
  %56 = load i32 addrspace(1)* %lsr.iv4, align 1, !noalias !31
  %57 = mul nsw i32 %56, %55
  %58 = add nsw i32 %57, %sum.01.i
  %lsr.iv.next = add i32 %lsr.iv, -1
  %scevgep6 = getelementptr i1 addrspace(1)* %lsr.iv45, i64 %43
  %59 = bitcast i1 addrspace(1)* %scevgep6 to i32 addrspace(1)*
  %scevgep11 = getelementptr i32 addrspace(1)* %lsr.iv10, i64 1
  %exitcond.i = icmp eq i32 %lsr.iv.next, 0
  br i1 %exitcond.i, label %._crit_edge.i, label %54

._crit_edge.i:                                    ; preds = %54, %scalar_kernel_entry.i
  %sum.0.lcssa.i = phi i32 [ 0, %scalar_kernel_entry.i ], [ %58, %54 ]
  %60 = add nsw i32 %50, %53
  %61 = sext i32 %60 to i64
  %62 = getelementptr inbounds i32 addrspace(1)* %explicit_2, i64 %61
  store i32 %sum.0.lcssa.i, i32 addrspace(1)* %62, align 1, !noalias !31
  %dim_0_inc_ind_var.i = add nuw nsw i64 %dim_0_ind_var.i, 1
  %dim_0_inc_tid.i = add nuw nsw i64 %dim_0_tid.i, 1
  %dim_0_cmp.to.max.i = icmp eq i64 %dim_0_inc_ind_var.i, %scalar.size.i
  br i1 %dim_0_cmp.to.max.i, label %dim_0_exit.i, label %scalar_kernel_entry.i

dim_0_exit.i:                                     ; preds = %._crit_edge.i
  %dim_1_inc_ind_var.i = add nuw nsw i64 %dim_1_ind_var.i, 1
  %dim_1_inc_tid.i = add nuw nsw i64 %dim_1_tid.i, 1
  %lsr.iv.next8 = add i32 %lsr.iv7, %explicit_3
  %dim_1_cmp.to.max.i = icmp eq i64 %dim_1_inc_ind_var.i, %LocalSize_1
  br i1 %dim_1_cmp.to.max.i, label %__MatrixMultKernel_separated_args.exit, label %dim_0_pre_head.i

__MatrixMultKernel_separated_args.exit:           ; preds = %dim_0_exit.i, %scalarIf.i
  ret void
}

attributes #0 = { nounwind "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "stack-protector-buffer-size"="8" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { nounwind readnone "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "stack-protector-buffer-size"="8" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #2 = { alwaysinline nounwind "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "stack-protector-buffer-size"="8" "unsafe-fp-math"="false" "use-soft-float"="false" }

!opencl.kernels = !{!0}
!opencl.enable.FP_CONTRACT = !{}
!opencl.spir.version = !{!7}
!opencl.ocl.version = !{!7}
!opencl.used.extensions = !{!8}
!opencl.used.optional.core.features = !{!8}
!opencl.compiler.options = !{!8}
!llvm.ident = !{!9}
!opencl.kernel_info = !{!10}
!opencl.module_info_list = !{!27}
!llvm.functions_info = !{}
!opencl.functions_stats = !{}
!opencl.stat_descriptions = !{}
!opencl.module_stat_info = !{}

!0 = !{void (i32 addrspace(1)*, i32 addrspace(1)*, i32 addrspace(1)*, i32, i8 addrspace(3)*, { i64, [3 x i64], [3 x i64], [2 x [3 x i64]], [3 x i64], {}*, {}* }*, i64*, [4 x i64], i8*, {}*)* @__MatrixMultKernel_separated_args, !1, !2, !3, !4, !5, !6}
!1 = !{!"kernel_arg_addr_space", i32 1, i32 1, i32 1, i32 0}
!2 = !{!"kernel_arg_access_qual", !"none", !"none", !"none", !"none"}
!3 = !{!"kernel_arg_type", !"int*", !"int*", !"int*", !"int"}
!4 = !{!"kernel_arg_base_type", !"int*", !"int*", !"int*", !"int"}
!5 = !{!"kernel_arg_type_qual", !"", !"", !"", !""}
!6 = !{!"kernel_arg_name", !"Md", !"Nd", !"Pd", !"width"}
!7 = !{i32 1, i32 2}
!8 = !{}
!9 = !{!"clang version 3.6.2 "}
!10 = !{void (i32 addrspace(1)*, i32 addrspace(1)*, i32 addrspace(1)*, i32, i8 addrspace(3)*, { i64, [3 x i64], [3 x i64], [2 x [3 x i64]], [3 x i64], {}*, {}* }*, i64*, [4 x i64], i8*, {}*)* @__MatrixMultKernel_separated_args, !11}
!11 = !{!12, !13, !14, !15, !16, !17, !18, !19, !20, !21, !22, !23, !24, !25, !26}
!12 = !{!"local_buffer_size", i32 0}
!13 = !{!"barrier_buffer_size", i32 0}
!14 = !{!"kernel_execution_length", i32 170}
!15 = !{!"max_wg_dimensions", i32 2}
!16 = !{!"kernel_has_barrier", i1 false}
!17 = !{!"kernel_has_global_sync", i1 false}
!18 = !{!"no_barrier_path", i1 true}
!19 = !{!"vectorized_kernel", null}
!20 = !{!"vectorized_width", i32 4}
!21 = !{!"kernel_wrapper", void (i8*, i64*, {}*)* @MatrixMultKernel}
!22 = !{!"scalarized_kernel", null}
!23 = !{!"block_literal_size", null}
!24 = !{!"private_memory_size", i32 0}
!25 = !{!"vectorization_dimension", i32 0}
!26 = !{!"can_unite_workgroups", i1 true}
!27 = !{!28, !29, !30}
!28 = !{!"global_variable_total_size", i64 0}
!29 = !{!"gen_addr_space_pointer_counter", null}
!30 = !{!"gen_addr_space_pointer_warnings"}
!31 = !{!32}
!32 = distinct !{!32, !33, !"__MatrixMultKernel_separated_args: %pWorkDim"}
!33 = distinct !{!33, !"__MatrixMultKernel_separated_args"}
